\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{parskip}

\title{\Large Kernel methods in machine learning --- Homework 1}
\author{Hugo Cisneros}
\date{March 13th, 2019}

\begin{document}
    \maketitle

    \section*{Exercise 1. Kernels}

    \paragraph{1.1} $K$ is symmetric since 
    \begin{align*}
        \forall x,y \in \mathbb{R}^2 K(y, x) = \cos(-(x-y)) = \cos(x - y) = 
        K(x, y)
    \end{align*}
    and because for all sequences of $(x_i)$ and $(a_i)$ of $\mathbb{R}$
    \begin{align*}
        \tiny
        \sum_i\sum_j a_i a_j \cos(x_i - x_j) &= \sum_i\sum_j a_i a_j (\cos x_i
        \cos x_j + \sin x_i \sin x_j)\\
        &= \left(\sum_i a_i \cos x_i\right)^2 + \left(\sum_i a_i 
        \sin x_i\right)^2 \geq 0
    \end{align*}
    $K$ is p.d.

    \paragraph{1.2}
    $K$ is clearly symmetric and $\forall x, y \in \mathcal{X}, $
    \begin{align*}
        K(x,y) &= \frac{1}{1 - x^Ty}\\
        & = \lim_{n\rightarrow\infty} \sum_{k=0}^n (x^T y)^k \tag{converges 
        because by C-S $|x^Ty|
        \leq \lVert x \rVert_2\lVert y\rVert_2 < 1$}
    \end{align*}
    Since all powers of the linear kernel are p.d. kernels, and a sum of p.d. 
    kernels is a p.d. kernel, the above sum is a p.d. kernel for all $n$.
    Therefore, $K$ is p.d. as a point-wise limit of a sequence of p.d. kernels. 
    \paragraph{1.3} The kernel $K$ is symmetric because the intersection is a 
    commutative operation. 

    \begin{align*}
        \sum_{i=1}^n a_i a_j (P(A_i \cap A_j) - P(A_i) P(A_j)) &= 
        \sum_{i=1}^n a_i a_j (\mathbb{E}[\mathds{1}_{A_i \cap A_j}] - 
        \mathbb{E}[\mathds{1}_{A_i}] \mathbb{E}[\mathds{1}_{A_j}])\\
        &= \mathbb{E}\left[\sum_{i=1}^n a_i a_j \mathds{1}_{A_i}\mathds{1}_{A_j} 
        \right] - \sum_{i=1}^n \mathbb{E}[a_i\mathds{1}_{A_i}] 
        \mathbb{E}[a_j\mathds{1}_{A_j}]\\
        &= \mathbb{E}\left[\left(\sum_{i=1}^n a_i\mathds{1}_{A_i}\right)^2
        \right] - \left(\sum_{i=1}^n a_i \mathbb{E}[\mathds{1}_{A_i}] \right)^2
    \end{align*}
    By Jensen's inequality, the above quantity is positive because the function
    $\phi: (X_1, ..., X_n) \mapsto (\sum_{i=1}^n a_i X_i)^2$ is convex. 
    Therefore, \textbf{$K$ is p.d.}.
    \paragraph{1.4}
    \paragraph{1.5}

    \section*{Exercise 2. RKHS}
    \paragraph{2.1} \textbf{$\alpha K_1 + \beta K_2$ is p.d.} as sum of p.d. 
    kernels and because $\alpha$ and $\beta$ are positive scalars. Let 
    $\mathcal{H}_1$ and $\mathcal{H}_2$ be their respective RKHS.

    Since, $K_1$ and $K_2$ are p.d., by Aronszajn's theorem there exist 
    $\Phi_1, \Phi_2$ mappings from $\mathcal{X}$ to $\mathcal{H}_1$ and 
    $\mathcal{H}_2$ such that $\forall x,y \in \mathbb{R}^2$
    \begin{align*}
        K(x,y) &\triangleq \alpha K_1(x,y) + \beta K_2(x, y)\\
        &= \langle \sqrt{\alpha}\Phi_1(x),\sqrt{\alpha}\Phi_1(y) 
        \rangle_{\mathcal{H}_1} + \langle\sqrt{\beta} \Phi_2(x),
        \sqrt{\beta}\Phi_2(y) \rangle_{\mathcal{H}_2}\\
        & = \langle \Phi(x), \Phi(y) \rangle_{\mathcal{H}}
    \end{align*}
    Where $\langle.,.\rangle_{\mathcal{H}}$ is defined above and
    \begin{align*}
        \Phi & :\mathcal{X} \rightarrow \mathcal{H} \triangleq \sqrt{\alpha}
        \mathcal{H}_1 + \sqrt{\beta} \mathcal{H}_2\\
        &\quad x\mapsto \sqrt{\alpha} \Phi_1(x) + \sqrt{\beta} \Phi_2(x)
    \end{align*}
    $\langle.,.\rangle_{\mathcal{H}}$ is clearly an inner product. And for any
    $(x_n)_{n\in\mathbb{N}}$ sequence of $\mathcal{H}$, we have that 
    $\forall n \in \mathbb{N},x_n = \sqrt{\alpha} x_{1,n} + \sqrt{\beta} 
    x_{2,n}$ and $\forall n,m \in \mathbb{N}$
    \begin{align*}
        \lVert x_n - x_m \rVert^2_\mathcal{H} &= \lVert \sqrt{\alpha} (x_{1,n} - 
        x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m})\rVert^2_\mathcal{H}\\
        &= \langle \sqrt{\alpha} (x_{1,n} - 
        x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m}), \sqrt{\alpha} (x_{1,n} - 
        x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m}) \rangle_{\mathcal{H}}\\
        &= \alpha \langle x_{1,n} - x_{1,m}, x_{1,n} - x_{1,m} 
        \rangle_{\mathcal{H}_1} + \beta \langle x_{2,n} - x_{2,m}, x_{2,n} - 
        x_{2,m}  \rangle_{\mathcal{H}_2} \tag{by definition of 
        $\langle.,.\rangle_{\mathcal{H}}$}\\
        &= \alpha \lVert x_{1,n} - x_{1,m} \rVert^2_{\mathcal{H}_1} +
        \beta \lVert x_{2,n} - x_{2,m} \rVert^2_{\mathcal{H}_2}
    \end{align*}
    Therefore, if $(x_n)$ is a Cauchy sequence, $(x_{1,n})$ and  $(x_{2,n})$ 
    are also  Cauchy sequences. Because $\mathcal{H}_1$ and $\mathcal{H}_2$ 
    are Hilbert spaces, the two sequences converge and $(x_n)$ converges. 
    \textbf{$\mathcal{H}$ is a Hilbert space}.


    Let's consider the functions $K_x : t \mapsto K(x,t) = \alpha K_1(x,t) + 
    \beta K_2(x, t)$ for $x \in \mathcal{X}$

    Let, $f\in \mathcal{H}$, $x\in \mathcal{X}$. $f$ can be written $f = 
    \sqrt{\alpha}f_1 + \sqrt{\beta}f_2$ with $f_1 \in \mathcal{H}_1$ and
    $f_2 \in \mathcal{H}_2$. 
    \begin{align*}
        f(x) &= \sqrt{\alpha}f_1(x) + \sqrt{\beta}f_2(x)\\
        &= \sqrt{\alpha}\langle f_1, K_{1,x}\rangle_{\mathcal{H}_1} + 
        \sqrt{\beta}\langle f_2, K_{2,x}\rangle_{\mathcal{H}_2}\\
        &=\langle f, K_x \rangle_{\mathcal{H}}
    \end{align*}
    \paragraph{2.2}

    \section*{Exercise 3. RKHS}
    \paragraph{3.1} \textbf{ $\mathcal{H}$ is Hilbert: }
    
    $\mathcal{H}$ is a vector space of functions. $\langle ., .
    \rangle_\mathcal{H}$ is a symmetric bilinear form verifying $\forall f, 
    \langle f, f \rangle_\mathcal{H} \geq 0$.
    
    Since $f$ is absolutely continuous, it has a derivative almost everywhere 
    and the following equality holds $\forall x \in [0, 1]$
    \begin{align*}
        |f(x)|^2Â &= \left| f(0) + \int_0^x f'(x)\text{d}x \right|^2\\
        &= \left| \int_0^x f'(x)\text{d}x \right|^2 \tag{$f(0) = 0$}\\ 
        &\leq x\cdot\int_0^x f'(u)^2\text{d}u = x \cdot
        \langle f, f \rangle_\mathcal{H}
    \end{align*}
    $\langle f, f \rangle_\mathcal{H} \implies f = 0$ and $\langle ., .
    \rangle_\mathcal{H}$ is therefore and inner product. \textbf{$\mathcal{H}$
    is a pre-Hilbert space with $\langle ., . \rangle_\mathcal{H}$ as inner 
    product}.

    Let $(f_n)$ a Cauchy sequence of $\mathcal{H}$. $(f'_n)$ is a Cauchy 
    sequence of  $L^2([0,1])$ which is complete. Therefore it converges to a 
    function $g \in L^2([0,1])$.

    Since for all $(n,m)$, $x \in [0, 1]$, 
    $|f_n(x) - f_m(x)|^2\leq x \cdot \lVert f_n - f_m \rVert_\mathcal{H}^2$, the
    sequence $f_n(x)$ is Cauchy for any $x$ and converges to a real number 
    $f(x)$. 
    And since $f(x) = \lim_\infty f_n(x) = \lim_\infty \int_0^xf'_n(u)\text{d}u
    =  \int_0^x g(u)\text{d}u$, f is absolutely continuous and $f' = g$ almost
    everywhere. Moreover, $f'\in L^2([0, 1])$ and $f(0) = \lim_\infty f_n(0) 
    = 0$. 
     
    Finally, $\lVert f_n - f \rVert_\mathcal{H} = \lVert f'_n - g 
    \rVert_{L^2([0,1])} \rightarrow 0$ and $f\in \mathcal{H}$.
    \textbf{$\mathcal{H}$ is complete, therefore $\mathcal{H}$ is a Hilbert
    space}.
    \vspace{1cm}

    \textbf{Reproducing property: }

    We will now show that $\mathcal{H}$ is the RKHS with corresponding kernel 
    $K: (x, y) \rightarrow \min(x, y)$ on $[0, 1]$.

    For $x\in [0, 1]$, the function $K_x = \min(x,\cdot)$ is differentiable 
    except on the singleton ${x}$ which has a null measure, it is absolutely 
    continuous. Its derivative is square integrable and $\min(x,0) = 0$, 
    \textbf{therefore $K_x$ is in $\mathcal{H}$ for all $x$}.
\end{document}