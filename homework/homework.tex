\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{parskip}
\usepackage{mathtools}

\title{\Large Kernel methods in machine learning --- Homework 1}
\author{Hugo Cisneros}
\date{March 13th, 2019}

\begin{document}
    \maketitle

    \section*{Exercise 1. Kernels}

    \paragraph{1.1} $K$ is symmetric since 
    \begin{align*}
        \forall x,y \in \mathbb{R}^2 K(y, x) = \cos(-(x-y)) = \cos(x - y) = 
        K(x, y)
    \end{align*}
    and because for all sequences of $(x_i)$ and $(a_i)$ of $\mathbb{R}$
    \begin{align*}
        \tiny
        \sum_i\sum_j a_i a_j \cos(x_i - x_j) &= \sum_i\sum_j a_i a_j (\cos x_i
        \cos x_j + \sin x_i \sin x_j)\\
        &= \left(\sum_i a_i \cos x_i\right)^2 + \left(\sum_i a_i 
        \sin x_i\right)^2 \geq 0
    \end{align*}
    $K$ is p.d.

    \paragraph{1.2}
    $K$ is clearly symmetric and $\forall x, y \in \mathcal{X}, $
    \begin{align*}
        K(x,y) &= \frac{1}{1 - x^Ty}\\
        & = \lim_{n\rightarrow\infty} \sum_{k=0}^n (x^T y)^k \tag{converges 
        because by C-S $|x^Ty|
        \leq \lVert x \rVert_2\lVert y\rVert_2 < 1$}
    \end{align*}
    Since all powers of the linear kernel are p.d. kernels, and a sum of p.d. 
    kernels is a p.d. kernel, the above sum is a p.d. kernel for all $n$.
    Therefore, $K$ is p.d. as a point-wise limit of a sequence of p.d. kernels. 
    \paragraph{1.3} The kernel $K$ is symmetric because the intersection is a 
    commutative operation. 

    \begin{align*}
        \sum_{i=1}^n a_i a_j (P(A_i \cap A_j) - P(A_i) P(A_j)) &= 
        \sum_{i=1}^n a_i a_j (\mathbb{E}[\mathds{1}_{A_i \cap A_j}] - 
        \mathbb{E}[\mathds{1}_{A_i}] \mathbb{E}[\mathds{1}_{A_j}])\\
        &= \mathbb{E}\left[\sum_{i=1}^n a_i a_j \mathds{1}_{A_i}\mathds{1}_{A_j} 
        \right] - \sum_{i=1}^n \mathbb{E}[a_i\mathds{1}_{A_i}] 
        \mathbb{E}[a_j\mathds{1}_{A_j}]\\
        &= \mathbb{E}\left[\left(\sum_{i=1}^n a_i\mathds{1}_{A_i}\right)^2
        \right] - \left(\sum_{i=1}^n a_i \mathbb{E}[\mathds{1}_{A_i}] \right)^2
    \end{align*}
    By Jensen's inequality, the above quantity is positive because the function
    $\phi: (X_1, ..., X_n) \mapsto (\sum_{i=1}^n a_i X_i)^2$ is convex. 
    Therefore, \textbf{$K$ is p.d.}.
    \paragraph{1.4} Let $x, y \in \mathcal{X}$
    \paragraph{1.5}

    \section*{Exercise 2. RKHS}
    \paragraph{2.1} \textbf{$\alpha K_1 + \beta K_2$ is p.d.} as sum of p.d. 
    kernels and because $\alpha$ and $\beta$ are positive scalars. Let 
    $\mathcal{H}_1$ and $\mathcal{H}_2$ be their respective RKHS.

    
    % Since, $K_1$ and $K_2$ are p.d., by Aronszajn's theorem there exist 
    % $\Phi_1, \Phi_2$ mappings from $\mathcal{X}$ to $\mathcal{H}_1$ and 
    % $\mathcal{H}_2$ such that $\forall x,y \in \mathbb{R}^2$
    % \begin{align*}
    %     K(x,y) &\triangleq \alpha K_1(x,y) + \beta K_2(x, y)\\
    %     &= \langle \sqrt{\alpha}\Phi_1(x),\sqrt{\alpha}\Phi_1(y) 
    %     \rangle_{\mathcal{H}_1} + \langle\sqrt{\beta} \Phi_2(x),
    %     \sqrt{\beta}\Phi_2(y) \rangle_{\mathcal{H}_2}\\
    %     & = \langle \Phi(x), \Phi(y) \rangle_{\mathcal{H}}
    % \end{align*}
    % Where $\langle.,.\rangle_{\mathcal{H}}$ is defined above and
    % \begin{align*}
    %     \Phi & :\mathcal{X} \rightarrow \mathcal{H} \triangleq \sqrt{\alpha}
    %     \mathcal{H}_1 + \sqrt{\beta} \mathcal{H}_2\\
    %     &\quad x\mapsto \sqrt{\alpha} \Phi_1(x) + \sqrt{\beta} \Phi_2(x)
    % \end{align*}
    % $\langle.,.\rangle_{\mathcal{H}}$ is clearly an inner product. And for any
    % $(x_n)_{n\in\mathbb{N}}$ sequence of $\mathcal{H}$, we have that 
    % $\forall n \in \mathbb{N},x_n = \sqrt{\alpha} x_{1,n} + \sqrt{\beta} 
    % x_{2,n}$ and $\forall n,m \in \mathbb{N}$
    % \begin{align*}
    %     \lVert x_n - x_m \rVert^2_\mathcal{H} &= \lVert \sqrt{\alpha} (x_{1,n} - 
    %     x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m})\rVert^2_\mathcal{H}\\
    %     &= \langle \sqrt{\alpha} (x_{1,n} - 
    %     x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m}), \sqrt{\alpha} (x_{1,n} - 
    %     x_{1,m}) + \sqrt{\beta} (x_{2,n} -  x_{2,m}) \rangle_{\mathcal{H}}\\
    %     &= \alpha \langle x_{1,n} - x_{1,m}, x_{1,n} - x_{1,m} 
    %     \rangle_{\mathcal{H}_1} + \beta \langle x_{2,n} - x_{2,m}, x_{2,n} - 
    %     x_{2,m}  \rangle_{\mathcal{H}_2} \tag{by definition of 
    %     $\langle.,.\rangle_{\mathcal{H}}$}\\
    %     &= \alpha \lVert x_{1,n} - x_{1,m} \rVert^2_{\mathcal{H}_1} +
    %     \beta \lVert x_{2,n} - x_{2,m} \rVert^2_{\mathcal{H}_2}
    % \end{align*}
    % Therefore, if $(x_n)$ is a Cauchy sequence, $(x_{1,n})$ and  $(x_{2,n})$ 
    % are also  Cauchy sequences. Because $\mathcal{H}_1$ and $\mathcal{H}_2$ 
    % are Hilbert spaces, the two sequences converge and $(x_n)$ converges. 
    % \textbf{$\mathcal{H}$ is a Hilbert space}.


    % Let's consider the functions $K_x : t \mapsto K(x,t) = \alpha K_1(x,t) + 
    % \beta K_2(x, t)$ for $x \in \mathcal{X}$

    % Let, $f\in \mathcal{H}$, $x\in \mathcal{X}$. $f$ can be written $f = 
    % \sqrt{\alpha}f_1 + \sqrt{\beta}f_2$ with $f_1 \in \mathcal{H}_1$ and
    % $f_2 \in \mathcal{H}_2$. 
    % \begin{align*}
    %     f(x) &= \sqrt{\alpha}f_1(x) + \sqrt{\beta}f_2(x)\\
    %     &= \sqrt{\alpha}\langle f_1, K_{1,x}\rangle_{\mathcal{H}_1} + 
    %     \sqrt{\beta}\langle f_2, K_{2,x}\rangle_{\mathcal{H}_2}\\
    %     &=\langle f, K_x \rangle_{\mathcal{H}}
    % \end{align*}
    \paragraph{2.2}

    \section*{Exercise 3. RKHS}
    \paragraph{3.1} \textbf{ $\mathcal{H}$ is Hilbert: }
    
    $\mathcal{H}$ is a vector space of functions. $\langle ., .
    \rangle_\mathcal{H}$ is a symmetric bilinear form verifying $\forall f, 
    \langle f, f \rangle_\mathcal{H} \geq 0$.
    
    Since $f$ is absolutely continuous, it has a derivative almost everywhere 
    and the following equality holds $\forall x \in [0, 1]$
    \begin{align*}
        |f(x)|^2 &= \left| f(0) + \int_0^x f'(x)\text{d}x \right|^2\\
        &= \left| \int_0^x f'(x)\text{d}x \right|^2 \tag{$f(0) = 0$}\\ 
        &\leq x\cdot\int_0^x f'(u)^2\text{d}u = x \cdot
        \langle f, f \rangle_\mathcal{H}
    \end{align*}
    $\langle f, f \rangle_\mathcal{H} \implies f = 0$ and $\langle ., .
    \rangle_\mathcal{H}$ is therefore and inner product. \textbf{$\mathcal{H}$
    is a pre-Hilbert space with $\langle ., . \rangle_\mathcal{H}$ as inner 
    product}.

    Let $(f_n)$ a Cauchy sequence of $\mathcal{H}$. $(f'_n)$ is a Cauchy 
    sequence of  $L^2([0,1])$ which is complete. Therefore it converges to a 
    function $g \in L^2([0,1])$.

    Since for all $(n,m)$, $x \in [0, 1]$, 
    $|f_n(x) - f_m(x)|^2\leq x \cdot \lVert f_n - f_m \rVert_\mathcal{H}^2$, the
    sequence $f_n(x)$ is Cauchy for any $x$ and converges to a real number 
    $f(x)$. 
    And since $f(x) = \lim_\infty f_n(x) = \lim_\infty \int_0^xf'_n(u)\text{d}u
    =  \int_0^x g(u)\text{d}u$, $f$ is absolutely continuous and $f' = g$ almost
    everywhere. Moreover, $f'\in L^2([0, 1])$ and $f(0) = \lim_\infty f_n(0) 
    = 0$. 
     
    Finally, $\lVert f_n - f \rVert_\mathcal{H} = \lVert f'_n - g 
    \rVert_{L^2([0,1])} \xrightarrow[n \rightarrow +\infty]{} 0$ and $f\in 
    \mathcal{H}$. \textbf{$\mathcal{H}$ is complete, therefore $\mathcal{H}$ is 
    a Hilbert space}.
    \vspace{1cm}

    \textbf{Reproducing property: }

    We will now show that $\mathcal{H}$ is the RKHS with corresponding kernel 
    $K: (x, y) \rightarrow \min(x, y)$ on $[0, 1]^2$.

    For $x\in [0, 1]$, the function $K_x = \min(x,\cdot)$ is differentiable 
    except on the singleton $\{x\}$ which has a null measure, it is absolutely 
    continuous. Its derivative is square integrable and $\min(x,0) = 0$, 
    \textbf{therefore $K_x$ is in $\mathcal{H}$ for all $x$}.

    For any $x\in [0, 1]$ and $f \in \mathcal{H}$
    \begin{align*}
        \langle f, K_x \rangle_\mathcal{H} = \int_0^1 f'(u)K'_x(u)\text{d}u = 
        \int_0^x f'(u)\text{d}u = f(x)
    \end{align*}
    Therefore, \textbf{$K$ is the r.k. of the RKHS $\mathcal{H}$}.

    \paragraph{3.2} The above demonstration also shows that 
    \textbf{$\mathcal{H}$ is a Hilbert space} by remarking that 
    \begin{align*}
        f(1) = \lim_\infty f_n(1) = 1
    \end{align*} for $(f_n)$ Cauchy sequence of elements of 
    $\mathcal{H}$.
    
    We will now show that the r.k. corresponding to this Hilbert space is $K: 
    (x, y) \mapsto \min(x,y) - xy$. 

    For $x\in[0, 1]$, the function $K_x$ has a derivative everywhere except on 
    $\{x\}$ and its derivative is square integrable. Moreover, $K_x(0) = 0$ and 
    $K_x(1) = 0 $, therefore \textbf{$K_x$ is in $\mathcal{H}$}.

    Let $x\in[0, 1]$ and $f\in \mathcal{H}$
    \begin{align*}
        \langle f, K_x \rangle_\mathcal{H} = \int_0^1 f'(u)K'_x(u)\text{d}u = 
        (1-x)\int_0^x f'(u)\text{d}u - x \int_x^1 f'(u)\text{d}u = f(x)
    \end{align*}

    Therefore \textbf{$K$ is the r.k. of the RKHS $\mathcal{H}$}.

    \paragraph{3.3} By the theorem on Green kernels, we have that $\mathcal{H}$
    is a RKHS that admits as r.k. the Green function of the operator $D^*D$, 
    where $D = \frac{\text{d}}{\text{d}x} + 1$
    To find the r.k. of $\mathcal{H}$ we need to solve in $K$
    \begin{align*}
        f(x) = \langle D^*DK_x, f\rangle_{L^2([0, 1])} = \langle DK_x, 
        Df\rangle_{L^2([0, 1])} = \langle K_x, f\rangle_{\mathcal{H}}
    \end{align*}
    Since $\forall x \in [0, 1]$
    \begin{align*}
        \int_0^1 K'_x(u)f'(u) + K_x(u)f(u)\text{d}u& = \left[K'_x(u)f(u)
        \right]^1_0 - \int_0^1K''_x(u)f(u)\text{d}u + 
        \int_0^1K_x(u)f(u)\text{d}u\\& = \int_0^1 (K_x(u) - K''_x(u))f(u)
        \text{d}u
    \end{align*}
    We can then write $D^*D = 1 - \frac{\text{d}^2}{\text{d}x^2}$. Let $x
    \in[0, 1]$, the Green function of the operator $D^*D$ solves the equation 
    \begin{align*}
        (D^*D) G(x, t) = G(x, t) - G''(x, t)  = \delta(x-t)
    \end{align*} therefore, the solutions have the form $t\mapsto 
    c_1 e^t + c_2 e^{-t}$. On $[0, x)$, the boundary conditions imply $c_1 = 
    -c_2$ and on $(x, 1]$, $c_1e = -c_2e^{-1}$. We have the following general 
    form of the Green function
    \begin{align*}
        G(x, t) = \begin{cases}
            A(x)\sinh(t)\\
            B(x)\sinh(1 - t)
        \end{cases}
    \end{align*}
    The continuity condition at $t=x$ gives $A(x)\sinh(x) = B(x)\sinh(1-x)$ and 
    the condition on the jump in derivative (obtained by 
    integrating the differential equation between $x-\epsilon$ and
     $x+\epsilon$ with $\epsilon \rightarrow 0$) gives $  A(x) \cosh(x) + 
     \cosh(1-x)B(x) = -1$

    Therefore, the solution is 
    \begin{align*}
    K_x(t) &= G(x, t) = \begin{cases}
        \frac{1}{\sinh(1)}\sinh(1-x)\sinh(t)  & \text{for } 0\leq t < x\\
        \frac{1}{\sinh(1)}\sinh(x)\sinh(1 - t)& \text{for } x < t\leq 1
    \end{cases}\\
    & = \min\left(\frac{1}{\sinh(1)}\sinh(1-x)\sinh(t), 
    \frac{1}{\sinh(1)}\sinh(x)\sinh(1 - t)\right) 
    \end{align*}
    \begin{align*}
        f(x) &= \langle D^* D G, f \rangle\\
        & = \langle  D G, Df \rangle  \\
        &= \int_0^x \frac{\sinh(1-x)}{\sinh 1} 
        (\sinh(u)f(u) + \cosh(u)f'(u ) )\text{d}u + \\
        &\quad \int_x^1  \frac{\sinh(x)}{\sinh 1} 
        (\sinh(1-u)f(u) - \cosh(1- u)f'(u))\text{d}u
    \end{align*}

    Since all functions $K_x$ are in $\mathcal{H}$, the kernel 
    \begin{align*}
        K(x, y) = \min\left(\frac{1}{\sinh(1)}\sinh(1-x)\sinh(y), 
        \frac{1}{\sinh(1)}\sinh(x)\sinh(1 - y)\right) 
    \end{align*}
    is the r.k. of $\mathcal{H}$.
    \section*{Exercise 4. Duality}
    \paragraph{4.a} The problem is a convex problem for which strong duality 
    holds because it is strictly feasible (e.g the point $f=0$ satisfies the
    inequality constraint). Therefore, there exist a dual parameter such that
    the problem is equivalent to 
    \begin{equation*}
        \begin{aligned}
        & \underset{f\in\mathcal{H}_\mathcal{K}}{\text{min}}
        & & \frac{1}{n}\sum_{i=1}^n \ell_{y_i}(f(x_i)) + \lambda \lVert f
        \rVert_{\mathcal{H}_\mathcal{K}}\\
        \end{aligned}
    \end{equation*}

    According to the representer theorem, the above problem has a solution of
    the form $\forall x\in \mathcal{X}$
    \begin{align*}
        f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)
    \end{align*}
    The optimization problem can be re-written as 
    \begin{equation*}
        \begin{aligned}
        & \underset{\alpha\in\mathbb{R}^n}{\text{min}}
        & & R(K\alpha) + \lambda \alpha^\top K \alpha\\
        \end{aligned}
    \end{equation*} 
    where 
    \begin{align*}
        R:& \ \mathbb{R}^n \rightarrow \mathbb{R}\\
        & ([K\alpha]_1, ..., [K\alpha]_n) \mapsto \frac{1}{n}\sum_{i=1}^n
        \ell_{y_i}([K\alpha]_i)
    \end{align*}

    \paragraph{4.b} From the definition
    \begin{align*}
        R^*(u) &= \sup_{x \in \mathbb{R}^n}\left( x^\top u - R(x) \right) \\
        &= \sup_{x \in \mathbb{R}^n} \frac{1}{n}\sum_{i=1}^n (nx_i u_i - 
        \ell_{y_i}(x_i))\\
        &= \frac{1}{n}\sum_{i=1}^n \ell^*_{y_i}(nu_i)
    \end{align*}
        
    \paragraph{4.c} The Laplacian of the problem is 
    \begin{align*}
        L(u, \alpha, \mu) = R(u) + \lambda \alpha^\top K \alpha + \mu^\top (
        K\alpha - u)
    \end{align*}
    The dual function is given by $g(\mu) = \inf_{u, \alpha} L(u, \alpha, \mu)$
    \begin{align*}
        R^*(u) + \inf(\lambda\alpha^\top K \alpha + \mu^\top K\alpha)
    \end{align*}
\end{document}