\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{hyperref}

\newtcbtheorem{Theorem}{Theorem}{
  enhanced,
  sharp corners,
  center,
  leftrule=1mm,
  width=.9\linewidth,
  attach boxed title to top left={
    yshifttext=-1mm
  },
  colback=white,
  colframe=black!50!white,
  fonttitle=\bfseries,
  boxed title style={
    sharp corners,
    size=small,
    colback=black!50!white,
    colframe=black!50!white,
  } 
}{thm}

\newtcbtheorem{Definition}{Definition}{
  enhanced,
  sharp corners,
  center,
  leftrule=1mm,
  width=.9\linewidth,
  attach boxed title to top left={
    yshifttext=-1mm
  },
  colback=white,
  colframe=black!25!white,
  fonttitle=\bfseries,
  boxed title style={
    sharp corners,
    size=small,
    colback=black!25!white,
    colframe=black!25!white,
  } 
}{def}

\newtcbtheorem{Proof}{Proof}{
  enhanced,
  sharp corners,
  colback=white,
  colframe=black!5!white,
  coltitle=black,
  fonttitle=\bfseries,
  breakable,
}{prf}


\title{Kernel Methods in Machine Learning - Course Notes}
\author{Hugo Cisneros}
\date{}

\begin{document}
\maketitle

\section{Kernels and RKHS}

\subsection{Positive Definite Kernels}

\begin{Definition}{}{}
    A kernel $K$ is  a comparison function $K: \mathcal{X}\times\mathcal{X}
 \rightarrow \mathbb{R}$.


    With $n$ data point $\{x_1, x_2, ..., x_n\}$ a $n \times n$ matrix 
    $\mathbf{K}$ can be defined by $\mathbf{K}_{ij} = K(x_i, x_j)$.

    A kernel $K$ is \textbf{positive definite} (p.d.) if it is 
    \textbf{symmetric} ($K(x, x') = K(x', x)$) and for all sets of $a$ and $x$
    \begin{align*}
        \boxed{\sum_i\sum_j a_i a_j K(x_i, x_j) \geq 0}
    \end{align*}
\end{Definition}

This is equivalent to the kernel matrix being \textbf{positive semi-definite}. 

\underline{Examples:}\begin{itemize}
    \item Kernel on $\mathbb{R}\times\mathbb{R}$  defined by 
    $K(x, x') = xx'$ is p.d. ($xx' = x'x$ and $ \sum_i\sum_j a_i a_j 
    K(x_i, x_j) = \left(\sum_i a_i x_i\right)^2 \geq 0$).
    \item Linear kernel ($K(x, x') = \langle x, x'\rangle_{\mathbb{R}^d}$) is 
    p.d 
    \item More generally for any set $\mathcal{X}$, and function $\Phi: 
    \mathcal{X} \rightarrow \mathbb{R}^d$, the kernel defined by $K(x,x') = 
    \langle \Phi(x), \Phi(x') \rangle_{\mathbb{R}^d}$ is p.d.
\end{itemize} 

\begin{Theorem}{Aronszajn, 1950}{aronszajn}
    $K$ is a p.d. kernel on the set $\mathcal{X}$ if and only if there exists a 
    \textbf{Hilbert space $\mathcal{H}$ and a mapping $\Phi : \mathcal{X} 
    \rightarrow \mathcal{H}$} such that, for any $x$, $x'$ in $\mathcal{X}$:
    \begin{align*}
        \boxed{K(x, x') = \langle \Phi(x), \Phi(x') \rangle_\mathcal{H}}
    \end{align*}
\end{Theorem}
\hyperref[prf:aronszajn]{\small Proof}.

(A Hilbert space is a vector space with an inner product and complete for the 
corresponding norm).

\subsection{Reproducing Kernel Hilbert Spaces (RKHS)}

Let $\mathcal{X}$ be a set and $\mathcal{H} \subset \mathbb{R}^\mathcal{X}$ a 
class of functions forming a Hilbert space. 
\begin{Definition}{Reproducing kernel}{}
    A kernel $K$ is called a \textbf{reproducing kernel} (r.k.) of $\mathcal{H}$ 
    if 
    \begin{itemize}
        \item $\mathcal{H}$ contains all functions of the form 
        \begin{align*}
           \boxed{\forall x \in \mathcal{X}, K_x: t \rightarrow K(x, t)}
        \end{align*}
        \item For every $x \in \mathcal{X}$ and $f\in \mathcal{H}$, 
        $\boxed{f(x) = \langle f, K_x \rangle_\mathcal{H} }$
    \end{itemize}
\end{Definition}

If there exists a r.k., $\mathcal{H}$ is called a RKHS.

\begin{Theorem}{Equivalent Definition of RKHS}{rkhs}
    $\mathcal{H}$ is a RKHS if and only if for any $x\in\mathcal{X}$, the 
    mapping
    \begin{align*}
        F: & \mathcal{H}\rightarrow \mathbb{R}\\
        & f \mapsto f(x)
    \end{align*}
    is \textbf{continuous}.
\end{Theorem}
\hyperref[prf:rkhs]{\small Proof}.

As a corollary, convergence in a RKHS implies point-wise convergence.


\begin{Theorem}{Uniqueness of RKHS}{unique_rkhs}
  If $\mathcal{H}$ is a RKHS, it has a \textbf{unique r.k.}, and a function $K$
  can be \textbf{the r.k of at most one RKHS}.
\end{Theorem}
\hyperref[prf:unique_rkhs]{\small Proof}.


\begin{Theorem}{}{pdrk}
  A function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is 
  \textbf{p.d. if and only if it is a r.k.}.
\end{Theorem}
\hyperref[prf:pdrk]{\small Proof}.


\subsection{Examples}
\subsubsection{Linear Kernel}

\begin{Definition}{}{}
  In $\mathbb{R}^d$, the linear kernel is defined by $K(x, y) = \langle x, y 
  \rangle_{\mathbb{R}^d}$
\end{Definition}

\begin{Theorem}{}{}
  
\end{Theorem}

%%---------------------------------
\section{Kernel tricks}

%%---------------------------------
\section{Kernel Methods: Supervised Learning}

%%---------------------------------
\section{Kernel Methods: Unsupervised Learning}

%%---------------------------------
\section{The Kernel Jungle}
\subsection{Green, Mercer, Herglotz, Bochner and friends}

\subsubsection{Green Kernel}

\begin{Theorem}{Green Kernel in dimension 1}{green1}
  The set defined by
  \begin{align*}
    \mathcal{H} = \left\{f: [0, 1] \rightarrow \mathbb{R}, \text{absolutely 
    continuous}, f' \in L^2([0, 1]), f(0) = 0 \right\}
  \end{align*}
  endowed with the inner product $\forall (f, g) \in \mathcal{F}^2 \langle f, g
  \rangle = \int_0^1 f'(u)g'(u)du$, 
  is a RKHS with with r.k.
  \begin{align*}
    \forall (x, y) \in [0, 1]^2, K(x, y) = \min(x, y)
  \end{align*}.
\end{Theorem}

\begin{Theorem}{General Green Kernel}{green_general}
  If $D$ is a differential operator on a class of functions of $\mathcal{H}$
  such that the inner product $\langle f, g \rangle_\mathcal{H} = \langle Df, Dg 
  \rangle_{L^2(\mathcal{X})}$

  Then $\mathcal{H}$ is a RKHS and admits for r.k. the Green function of the 
  operator $D^*D$
\end{Theorem}


\subsubsection{Mercer Kernels}

\begin{Definition}{Mercer Kernels}{}
  A kernel $K$ on a set $\mathcal{X}$ is called a Mercer kernel if: 
  \begin{itemize}
    \item $\mathcal{X}$ is a compact metric space (typically, a closed bounded
    subset of $\mathbb{R}^d$)
    \item $K: \mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}$ is a
    continuous p.d kernel (w.r.t the Borel topology)
  \end{itemize}
\end{Definition}

%%---------------------------------
\section{Open Problems and Research Topics}


%%---------------------------------
%%---------------------------------

\appendix

\section{Proofs}

\subsection{Kernels and RKHS}

\begin{Proof}{of Theorem \ref{thm:rkhs}}{rkhs}
  ($\Rightarrow$) If a r.k. exists in $\mathcal{H}$ then for any $(x,f) \in 
  \mathcal{X} \times \mathcal{H}$:
  \begin{align*}
    |f(x)| &= |\langle f, K_x \rangle_\mathcal{H} |\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot \lVert K_x \rVert_\mathcal{H}
           \tag{Cauchy-Schwarz}\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot K(x,x)^\frac{1}{2}
  \end{align*}
  Therefore, $f\in\mathcal{H} \rightarrow f(x) \in \mathbb{R}$ is a linear 
  continuous mapping because $F$ is linear and $\lim_{f\rightarrow 0} F(f)= 0$

  \vspace{10pt}
  ($\Leftarrow$) $F$ is continuous, by the Riesz representation theorem: there
  exists a unique $g_x \in \mathcal{H}$ such that $f(x) = \langle f, g_x 
  \rangle_\mathcal{H}$.

  The function $K: (x, y) \mapsto g_x(y)$ is then a r.k. for $\mathcal{H}$
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:unique_rkhs}}{unique_rkhs}
  (Uniqueness) If $K$ and $K'$ are two r.k. of a RKHS, then for any $x$
  \begin{align*}
    \lVert K_x - K'_x\rVert^2 = K_x(x) - K'_x(x) - K_x(x) + K'_x(x) = 0
  \end{align*}
  So $K_x = K'_X$

  % TODO The RKHS of a r.k is unique
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:pdrk}}{pdrk}
  ($\Leftarrow$) A r.k. is symmetric, and $\sum_{i,j} a_i a_j K(x_i, x_j)
   = \left\lVert \sum_i a_i K_{x_i} \right\rVert^2_\mathcal{H} \geq 0$

  \vspace{10pt}
  ($\Rightarrow$) Let $\mathcal{H}_0$ be the subspace spanned by the functions 
  $(K_x)_{x\in \mathcal{X}}$. If $f = \sum_i a_i K_{x_i}$ and $g =
  \sum_j b_j K_{y_j}$. Let (not an inner product yet)
  \begin{align*}
    \langle f, g\rangle_{\mathcal{H}_0} &= \sum_{i,j}a_i b_j K(x_i, y_j)\\
    &= \sum_i a_i g(x_i)\\
    &= \sum_j b_j f(y_j)
  \end{align*}
  ($\langle f,g \rangle_{\mathcal{H}_0}$ does not depend on the expansion of $f$
  or $g$) For any $x\in \mathcal{X}$ and $f \in \mathcal{H}_0$, $\langle f,K_x 
  \rangle_{\mathcal{H}_0} = f(x)$.

  \begin{align*}
    \lVert f \rVert^2_{\mathcal{H}_0} = \sum_{i,j} a_i a_j K(x_i, x_j) \geq 0
  \end{align*}
  And since Cauchy-Schwarz is valid, 
  \begin{align*}
    |f(x)| = |\langle f, K_x \rangle_{\mathcal{H}_0}| \leq \lVert f 
    \rVert_{\mathcal{H}_0}\cdot K(x,x)^{\frac{1}{2}}
  \end{align*}
  Therefore $\lVert f \rVert_{\mathcal{H}_0} = 0 \implies f = 0$. $\langle ., .
  \rangle$ is an inner product on $\mathcal{H}_0$. 

  For a Cauchy sequence $(f_n)_{n\geq 0}$, 
  \begin{align*}
    |f_m(x) - f_n(x)| \leq \lVert f_m - f_n \rVert_{\mathcal{H}_0} \cdot
    K(x, x)^{\frac{1}{2}}
  \end{align*}
  For any $x$ the sequence $(f_n(x))$ is Cauchy in $\mathbb{R}$ and therefore
  converges. 

  If the functions defined as the point-wise limits of Cauchy sequences are
  added $\mathcal{H}_0$, it becomes a Hilbert space with $K$ as r.k..
\end{Proof}

\begin{Proof}{of \hyperref[thm:aronszajn]{Aronszajn's theorem}}{aronszajn}
  If $K$ is p.d. over a set $\mathcal{X}$, it is the r.k. of a Hilbert space 
  $\mathcal{H}$. The mapping $\Phi$ is defined by $\forall x \in \mathcal{X}, 
  \quad \Phi(x) = K_x$.

  By the reproducing property 
  \begin{align*}
    \forall (x,y)\in \mathcal{X}^2,\quad \langle \Phi(x), \Phi(y) 
    \rangle_\mathcal{X} = \langle K_x, K_y \rangle_\mathcal{X} = K(x, y)
  \end{align*}
\end{Proof}


\subsection{The Kernel Jungle}

\begin{Proof}{}{}
  
\end{Proof}

\end{document}
