\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{boxedthms}
\usepackage{hyperref}
\usepackage{dsfont}

\title{Kernel Methods in Machine Learning - Course Notes}
\author{Hugo Cisneros}
\date{}

\begin{document}
\maketitle
\setcounter{tocdepth}{2}
\tableofcontents


\section{Kernels and RKHS}

\subsection{Positive Definite Kernels}

\begin{Definition}{}{}
    A kernel $K$ is  a comparison function $K: \mathcal{X}\times\mathcal{X}
 \rightarrow \mathbb{R}$.


    With $n$ data point $\{x_1, x_2, ..., x_n\}$ a $n \times n$ matrix 
    $\mathbf{K}$ can be defined by $\mathbf{K}_{ij} = K(x_i, x_j)$.

    A kernel $K$ is \textbf{positive definite} (p.d.) if it is 
    \textbf{symmetric} ($K(x, x') = K(x', x)$) and for all sets of $a$ and $x$
    \begin{align*}
        \boxed{\sum_i\sum_j a_i a_j K(x_i, x_j) \geq 0}
    \end{align*}
\end{Definition}

This is equivalent to the kernel matrix being \textbf{positive semi-definite}. 

\underline{Examples:}\begin{itemize}
    \item Kernel on $\mathbb{R}\times\mathbb{R}$  defined by 
    $K(x, x') = xx'$ is p.d. ($xx' = x'x$ and $ \sum_i\sum_j a_i a_j 
    K(x_i, x_j) = \left(\sum_i a_i x_i\right)^2 \geq 0$).
    \item Linear kernel ($K(x, x') = \langle x, x'\rangle_{\mathbb{R}^d}$) is 
    p.d 
    \item More generally for any set $\mathcal{X}$, and function $\Phi: 
    \mathcal{X} \rightarrow \mathbb{R}^d$, the kernel defined by $K(x,x') = 
    \langle \Phi(x), \Phi(x') \rangle_{\mathbb{R}^d}$ is p.d.
\end{itemize} 

\begin{Theorem}{Aronszajn, 1950}{aronszajn}
    $K$ is a p.d. kernel on the set $\mathcal{X}$ if and only if there exists a 
    \textbf{Hilbert space $\mathcal{H}$ and a mapping $\Phi : \mathcal{X} 
    \rightarrow \mathcal{H}$} such that, for any $x$, $x'$ in $\mathcal{X}$:
    \begin{align*}
        \boxed{K(x, x') = \langle \Phi(x), \Phi(x') \rangle_\mathcal{H}}
    \end{align*}
    \par \hfill \hyperref[prf:aronszajn]{\small Proof}.
\end{Theorem}


(A Hilbert space is a vector space with an inner product and complete for the 
corresponding norm).

\subsection{Reproducing Kernel Hilbert Spaces (RKHS)}

Let $\mathcal{X}$ be a set and $\mathcal{H} \subset \mathbb{R}^\mathcal{X}$ a 
class of functions forming a Hilbert space. 
\begin{Definition}{Reproducing kernel}{}
    A kernel $K$ is called a \textbf{reproducing kernel} (r.k.) of $\mathcal{H}$ 
    if 
    \begin{itemize}
        \item $\mathcal{H}$ contains all functions of the form 
        \begin{align*}
           \boxed{\forall x \in \mathcal{X}, K_x: t \rightarrow K(x, t)}
        \end{align*}
        \item For every $x \in \mathcal{X}$ and $f\in \mathcal{H}$, 
        $\boxed{f(x) = \langle f, K_x \rangle_\mathcal{H} }$
    \end{itemize}
\end{Definition}

If there exists a r.k., $\mathcal{H}$ is called a RKHS.

\begin{Theorem}{Equivalent Definition of RKHS}{rkhs}
    $\mathcal{H}$ is a RKHS if and only if for any $x\in\mathcal{X}$, the 
    mapping
    \begin{align*}
        F: & \mathcal{H}\rightarrow \mathbb{R}\\
        & f \mapsto f(x)
    \end{align*}
    is \textbf{continuous}.
    \par \hfill \hyperref[prf:rkhs]{\small Proof}.
\end{Theorem}


As a corollary, convergence in a RKHS implies point-wise convergence.


\begin{Theorem}{Uniqueness of RKHS}{unique_rkhs}
  If $\mathcal{H}$ is a RKHS, it has a \textbf{unique r.k.}, and a function $K$
  can be \textbf{the r.k of at most one RKHS}.
  \par \hfill \hyperref[prf:unique_rkhs]{\small Proof}.
\end{Theorem}



\begin{Theorem}{}{pdrk}
  A function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is 
  \textbf{p.d. if and only if it is a r.k.}.
  \par \hfill \hyperref[prf:pdrk]{\small Proof}.
\end{Theorem}



\subsection{Examples}

\subsubsection{Steps for finding the RKHS of a Kernel}
\begin{enumerate}
  \item Look for an \textbf{inner product} ($K(x, y) = \langle \Phi(x), \Phi(y)
  \rangle_\mathcal{H}$)
  \item Propose a \textbf{candidate RKHS} $\mathcal{H}$
  \item Check that the candidate $\mathcal{H}$ is a \textbf{Hilbert space} (
  inner product and complete)
  \item Check that $\mathcal{H}$ is \textbf{the RKHS}
  \begin{itemize}
    \item $\mathcal{H}$ contains all the functions $K_x : t \mapsto K(x,t)$
    \item For all $f \in \mathcal{H}$ and $x\in \mathcal{X}$, $f(x) = \langle
    f, K_x \rangle_\mathcal{H}$.
  \end{itemize}
\end{enumerate}

\subsubsection{Linear Kernel}

\begin{Definition}{Linear Kernel}{}
  In $\mathbb{R}^d$, the linear kernel is defined by $K(x, y) = \langle x, y 
  \rangle_{\mathbb{R}^d}$
\end{Definition}

\begin{Theorem}{RKHS of a linear Kernel}{lin_rkhs}
  The RKHS of the linear kernel is the set of linear functions of the form 
  $f_w(x) = \langle w,x \rangle_{\mathbb{R}^d}$ for $w \in \mathbb{R}^d$,
  endowed with the inner product $\langle f_w,f_v \rangle_\mathcal{H} = 
  \langle w,v \rangle_{\mathbb{R}^d}$
\end{Theorem}

\subsubsection{Polynomial Kernel}

\begin{Definition}{Polynomial Kernel}{}
  In $\mathbb{R}^d$, the polynomial kernel is defined by $K(x, y) = \langle x, y 
  \rangle_{\mathbb{R}^d}^2$
\end{Definition}

\begin{Theorem}{RKHS of a polynomial Kernel}{poly_rkhs}
  The RKHS $\mathcal{H}$ of the polynomial kernel is the set of quadratic 
  functions of the form $f_S(x) = x^T S x$ for $S \in \mathcal{S}^{d\times d}$ 
\end{Theorem}

\subsubsection{Properties of kernels} 

If $K_1$, $K_2$ are p.d. kernels, 
\begin{itemize}
  \item $K_1 + K_2$ is a p.d. kernel
  \item $K_1\cdot K_2$ is a p.d. kernel
  \item $cK_1$ for $c\geq 0$ is a p.d. kernel
  \item The point-wise limits of a sequence of p.d. kernels is a p.d kernel.
  \item $\exp(K_1)$ is a p.d. kernel
\end{itemize}

\textbf{Small norms in the RKHS space means slow variations in the original
space $\mathcal{X}$ with respect to the geometry defined by the kernel.}
%%---------------------------------
\section{Kernel tricks}

\subsection{Kernel trick}
\textbf{Statement:} All expression of vectors that can be written in terms of 
pairwise inner products can be transposed to a infinite dimensional space by 
replacing inner products with kernel evaluations. 

\subsection{Representer theorem}

\begin{Theorem}{Representer theorem}{representer}
  Let $\mathcal{X}$ a set with a p.d. kernel $K$ and corresponding RKHS 
  $\mathcal{H}$, $S = \{x_1, ..., x_n\} \subset \mathcal{X}$ a set of points of 
  $\mathcal{X}$.

  Let $\Phi : \mathbb{R}^{n+1} \rightarrow \mathbb{R}$ a function strictly 
  increasing w.r.t. the last variable.

  Any solution to the optimization problem
  \begin{align*}
    \min_{f\in \mathcal{H}} \Phi(f(x_1), ..., f(x_n), \lVert f
    \rVert_\mathcal{H})
  \end{align*}
  admits a representation in the form 
  \begin{align*}
    \forall x \in \mathcal{X}, f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)
  \end{align*}

  \par\hfill \hyperref[prf:representer]{\small Proof}
\end{Theorem}

One of the main consequences of the theorem is that problems of the form 
\begin{align*}
  \min_{f\in \mathcal{H}} \Phi(f(x_1), ..., f(x_n), \lVert f
  \rVert_\mathcal{H})
\end{align*}
can be re-written as 
\begin{align*}
  \min_{\alpha\in \mathbb{R}^n} \Phi([\mathbf{K}\alpha]_1, ..., 
  [\mathbf{K}\alpha]_n, \alpha^T \mathbf{K} \alpha)
\end{align*}
which is a n-dimensional optimization problem (instead of a possibly infinite 
dimensional one).

%%---------------------------------
\section{Kernel Methods: Supervised Learning}

\subsection{Kernel Ridge regression}

The problem can be described as minimizing a RKHS norm regularized MSE criterion
\begin{align*}
  \hat{f} = \arg\min_{f\in \mathcal{H}} \frac{1}{n}\sum_{i=1}^n(y_i - f(x_i))^2
  + \lambda\lVert f\rVert^2_\mathcal{H}
\end{align*} 

Effects of regularization: 
\begin{itemize}
  \item \textbf{Penalize non smooth functions} (avoid overfitting)
  \item \textbf{Simplify the solution} (representer theorem)
\end{itemize}

The problem can be re-written 
\begin{align*}
  \hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}^n}\frac{1}{n}
  (\mathbf{K}\alpha - y)^T (\mathbf{K}\alpha - y) + \lambda\alpha^T\mathbf{K}
  \alpha
\end{align*}

One solution is to take 
\begin{align*}
  \alpha = (\mathbf{K} + \lambda n \mathbf{I})^{-1} y
\end{align*}

(Uniqueness: If $\mathbf{K}$ is singular, all $\alpha + \varepsilon$ with 
$\varepsilon \in \text{Ker}(\mathbf{K})$ are solutions leading to the same 
function $f$.)

\subsection{Kernel logistic regression}

\begin{align*}
  \hat{f} = \arg\min_{f\in \mathcal{H}} \frac{1}{n}\sum_{i=1}^n \log\left(1 + 
  \exp(-y_if(x_i))\right) + \lambda\lVert f\rVert^2_\mathcal{H}
\end{align*} 

This problem can also be reformulated in terms of the Gram matrix of the kernel
and a parameter $\alpha$

\begin{align*}
  \min_{\alpha \in \mathbb{R}^n} J(\alpha) \triangleq \frac{1}{n}\sum_{i=1}^n 
  \log\left(1 + \exp(-y_i[\mathbf{K}\alpha]_i)\right) + \frac{\lambda}{2}
  \alpha^T\mathbf{K}\alpha
\end{align*} 

By writing and computing the terms of the Taylor expansion of $J$ near a point 
$\alpha_0$, we can explicitly solve the problem with Newton's method. 
\begin{align*}
  J_q(\alpha) = J(\alpha_0) + (\alpha - \alpha_0)^T \nabla J(\alpha_0) + 
  \frac{1}{2} (\alpha - \alpha_0)^T \nabla^2J(\alpha_0)(\alpha - \alpha_0)
\end{align*}
\begin{align*}
  \nabla J(\alpha) = \frac{1}{n}\mathbf{KP}(\alpha)y+ \lambda\mathbf{K}\alpha\\
  \nabla^2 J(\alpha) =  \frac{1}{n}\mathbf{KW}(\alpha)\mathbf{K} + \lambda 
  \mathbf{K}
\end{align*}
where $\mathbf{P}(\alpha) = \text{diag}(\ell'_{\text{logistic}}(y_i 
[\mathbf{K}\alpha]_i))$ and $\mathbf{W}(\alpha) = \text{diag}(
\ell''_{\text{logistic}}(y_i [\mathbf{K}\alpha]_i))$. By developing the 
approximation, we obtain the following equality
\begin{align*}
  2J_q(\alpha) = \frac{1}{n}(\mathbf{K}\alpha - z)^T \mathbf{w}
  (\mathbf{K}\alpha - z) + \lambda \alpha^T \mathbf{K}\alpha + C
\end{align*}
with $z = (\mathbf{K}\alpha_0 - \mathbf{W^{-1}}\mathbf{P}y)$. This is exactly 
the formulation of a weighted kernel ridge regression problem. This problem can
be iteratively solved by updating $W^t$ and $z^t$ until convergence (kernel
IRLS).

\subsection{Support vector machines (SVM)}

\begin{Definition}{Hinge loss}{}
  The Hinge loss is a function $\mathbb{R} \rightarrow \mathbb{R}_+$ defined by 
  \begin{align*}
    \varphi_\text{hinge}(u) = \max(0, 1-u) = \begin{cases}
      0\quad \text{if}\ u \geq 1\\
      1-u \quad \text{otherwise}
    \end{cases}
  \end{align*} 
\end{Definition}

\begin{Definition}{SVM problem}{}
  SVM is the large margin classifier that solves
  \begin{align*}
    \min_{f\in \mathcal{H}}\left\{ \frac{1}{n}\sum_{i=1}^n 
    \varphi_{\text{hinge}}(y_i f(x_i)) + \lambda \lVert f\rVert_\mathcal{H}^2
    \right\}
  \end{align*}
  It can be reformulated by using the Representer theorem as 
  \begin{align*}
    \min_{\alpha \in \mathbb{R}^n}\left\{ \frac{1}{n}\sum_{i=1}^n 
    \varphi_{\text{hinge}}(y_i [\mathbf{K}\alpha]_i) + \lambda \alpha^T
    \mathbf{K}\alpha \right\}
  \end{align*}
  Then, by introducing slack variables and using the definition of the Hinge 
  loss, the following formulation is obtained 
  \begin{align*}
    \hat{f}(x) = \sum_{i=1}^n \hat{\alpha}_i K(x_i, x)
  \end{align*}
  where $\hat{\alpha}$ solves
  \begin{equation*}
  \begin{aligned}
    &\min_{\alpha \in \mathbb{R}^n,\xi\in \mathbb{R}^n} & & \frac{1}{n}
    \sum_{i=1}^n \xi_i +\lambda \alpha^T \mathbf{K}\alpha \\
    &\text{s.t.} & & y_i [\mathbf{K}\alpha]_i+ \xi_i - 1 \geq 0, \quad 
    \forall i\\
    & & & \xi_i \geq 0 , \quad \forall i
  \end{aligned}
  \end{equation*}
\end{Definition}

%%---------------------------------
\section{Kernel Methods: Unsupervised Learning}

\subsection{Kernel K-means and spectral clustering}

The objective is similar to K-means, but transposed in the RKHS. Given data
points $x_1, ..., x_n$ and a p.d. kernel $K$ and RKHS $\mathcal{H}$ the
objective reads
\begin{align*}
  \min_{\substack{\mu_j \in \mathcal{H} \quad \forall j \leq k \\ s_i \in 
  \{1, ... k\}
  \quad \forall i \leq n}}\sum_{i=1}^n \lVert \varphi(x_i) - \mu_{s_i}
  \rVert^2_\mathcal{H}
\end{align*}.

\begin{Proposition}{}{center}
  The center of mass $\varphi_n = \frac{1}{n}\sum_{i=1}^n\varphi(x_i)$ solves
  the optimization problem
  \begin{align*}
    \min_{\mu \in \mathcal{H}}\sum_{i=1}^n\lVert \varphi(x_i) - \mu 
    \rVert^2_\mathcal{H}
  \end{align*}
  \par \hfill \hyperref[prf:center]{\small Proof.}
\end{Proposition}

\paragraph{Greedy (K-means) approach:}
\begin{description}
  \item[Centroid update] Given a centroid assignment, update the centroids 
  \begin{align*}
    \forall j,\quad \mu_j = \frac{1}{|C_j|}\sum_{i\in C_j}\varphi(x_i)
  \end{align*} 
  \item[Cluster assignment] For $\mu_1, ..., \mu_k$ centers of mass assign each 
  $x_i$ to the closest centroid. 
  \begin{align*}
    s_i \in \arg\,\min_{s\in \{1, ..., k\}} \lVert \varphi(x_i) - \mu_s 
    \rVert^2_{\mathcal{H}}
  \end{align*}  
\end{description}

\begin{Proposition}{}{}
  The equivalent objective to the kernel k-means algorithm is 
  \begin{align*}
    \max_{s_i\in{1,...,k} \forall i}\sum_{l=1}^k \frac{1}{|C_l|}\sum_{i,j \in 
    C_l} K(x_i, x_j)
  \end{align*}
\end{Proposition}
The above problem is a combinatorial optimization problem. The greedy algorithm 
(kernel K-means) approximates its solution but spectral clustering can also be
used. 

\textbf{Idea: } Introduce $\mathbf{A} \in \{0, 1\}^{n\times k}$ the binary 
assignment matrix and $\mathbf{D} \in \mathbb{R}^k$ a diagonal matrix with 
diagonal elements the inverse of cardinality of corresponding cluster. The 
objective becomes
\begin{align*}
  \max_{\mathbf{A}, \mathbf{D}} \ \text{tr}(\mathbf{D}^{1/2}\mathbf{A}^T
  \mathbf{K}\mathbf{A}\mathbf{D}^{1/2})
\end{align*}
such that the two matrices verify the properties implied by their definition. 
One can define $\mathbf{Z} = \mathbf{A}\mathbf{D}^{1/2}$ and the objective 
becomes
\begin{align*}
  \max_{\mathbf{Z}} \ \text{tr}(\mathbf{Z}^T \mathbf{K}\mathbf{Z})\quad
   \text{s.t.}\quad\mathbf{Z}^T\mathbf{Z} = \mathbf{I}
\end{align*} 
This can be solved by finding the eigenvectors of $\mathbf{K}$ with $k$ largest 
eigenvalues. Then, $\mathbf{Z}^*$ is used to find the best cluster assignment.

\subsection{Kernel PCA}
Assumption: data are centered w.r.t the kernel, i.e $ \frac{1}{n}\sum_{i=1}^n
\varphi(x_i) = 0$.
The orthogonal projection onto a direction $f$ in $\mathcal{H}$ is written 
$h_f(x) = \left\langle \varphi(x), \frac{f}{\lVert f\rVert_\mathcal{H}} 
\right\rangle_\mathcal{H}$

The empirical variance captured by a direction $f$ is 
\begin{align*}
  \text{Var}(h_f) = \frac{1}{n}\sum_{i=1}^n\frac{\langle \varphi(x_i), f 
  \rangle^2_\mathcal{H}}{\lVert f\rVert_\mathcal{H}^2} = 
  \frac{1}{n}\sum_{i=1}^n\frac{f(x_i)^2}{\lVert f\rVert_\mathcal{H}^2}
\end{align*}
and the $i$-th principal direction is 
\begin{align*}
  f_i = \arg\,\max_{f \perp {f_1, ..., f_{i-1}}} \text{Var}(h_f) = 
  \sum f(x_i)^2
   \quad 
  \text{s.t.}  \quad \lVert f\rVert_\mathcal{H} = 1
\end{align*}

In practice: 
\begin{enumerate}
  \item Center the Gram matrix
  \item Compute the required number of eigenvectors/values $(u_i, \Delta_i)$
  \item Normalize $\alpha_i = \frac{u_i}{\sqrt{\Delta_i}}$
  \item Project onto the $i$-th eigenvectors by computing $\mathbf{K}\alpha_i$
\end{enumerate}

%%---------------------------------
\section{The Kernel Jungle}

\subsection{Green, Mercer, Herglotz, Bochner and friends}

\subsubsection{Green Kernel}

\begin{Theorem}{Green Kernel in dimension 1}{green1}
  The set defined by
  \begin{align*}
    \mathcal{H} = \left\{f: [0, 1] \rightarrow \mathbb{R}, \text{absolutely 
    continuous}, f' \in L^2([0, 1]), f(0) = 0 \right\}
  \end{align*}
  endowed with the inner product $\forall (f, g) \in \mathcal{F}^2 \langle f, g
  \rangle = \int_0^1 f'(u)g'(u)du$, 
  is a RKHS with with r.k.
  \begin{align*}
    \forall (x, y) \in [0, 1]^2, K(x, y) = \min(x, y)
  \end{align*}.
  \par \hfill \hyperref[prf:green1]{\small Proof}.
\end{Theorem}

\begin{Definition}{Green functions}{}
  Consider the differential equation $f = Dg$ ($D$ differential operator).

  Solutions of the form $g(x) = \int_\mathcal{X} k(x,y)f(y) \text{d}y$ for some
  function $k$ that must satisfy 
  \begin{align*}
    \forall x \in \mathcal{X},\quad f(x)= Dg(x) = 
  \langle Dk_x, f\rangle_{L^2(\mathcal{X})}
  \end{align*}

  If $k$ exists, it is called the Green function of the operator $D$.
\end{Definition}

\begin{Theorem}{General Green Kernel}{green_general}
  If $D$ is a differential operator on a class of functions of $\mathcal{H}$
  such that the inner product $\langle f, g \rangle_\mathcal{H} = \langle Df, Dg 
  \rangle_{L^2(\mathcal{X})}$ make $\mathcal{H}$ a Hilbert space

  Then $\mathcal{H}$ is a RKHS and admits for r.k. the Green function of the 
  operator $D^*D$
\end{Theorem}


\subsubsection{Mercer Kernels}

\begin{Definition}{Mercer Kernels}{}
  A kernel $K$ on a set $\mathcal{X}$ is called a Mercer kernel if: 
  \begin{itemize}
    \item $\mathcal{X}$ is a compact metric space (typically, a closed bounded
    subset of $\mathbb{R}^d$)
    \item $K: \mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}$ is a
    continuous p.d kernel (w.r.t the Borel topology)
  \end{itemize}
\end{Definition}

% TODO

\subsubsection{Shift invariant Kernels}
\begin{Definition}{Fourier-Stieltjes coefficients}{}
  For a measure $\mu \in M(\mathbb{T})$ the set of the finite complex Borel 
  measures of the torus $[0, 2\pi]$, the Fourier-Stieltjes coefficients of 
  $\mu$ is the sequence
  \begin{align*}
    \forall n \in \mathbb{Z}, \quad \hat{\mu}(n) = \frac{1}{2\pi}
    \int_\mathbb{T}e^{-int}\text{d}\mu(t)
  \end{align*}
  (It is an extension of Fourier transform for integrable functions to measures)
\end{Definition}

\begin{Definition}{Shift invariant kernels on $\mathbb{Z}$}
  A kernel $K: \mathbb{Z}\times \mathbb{Z} \rightarrow\mathbb{R}$ is called 
  shift invariant (or translation invariant, t.i.) if it only depends on the 
  difference between its arguments, i.e. 
  \begin{align*}
    \forall x, y \in \mathbb{Z}, \quad K(x, y) = a_{x-y}
  \end{align*}
  For a sequence $(a_n)_{n\in\mathbb{Z}}$. The sequence is called p.d. if the 
  corresponding kernel is p.d..
\end{Definition}

\begin{Theorem}{Herglotz}{herglotz}
  A sequence $(a_n)_{n\in\mathbb{Z}}$ is p.d. iff it is the Fourier-Stieltjes 
  transform of a positive measure $\mu \in M(\mathbb{T})$
\end{Theorem}

Examples:
\begin{itemize}
  \item Diagonal kernel: 
  \begin{align*}
    \mu = \text{d}t, \quad a_n = \hat{\mu}(n)=  \frac{1}{2\pi}\int_\mathbb{T}
    e^{-int}\text{d}t = \begin{cases}
      1 & \text{if } n=0,\\
      0 & \text{otherwise}
    \end{cases}
  \end{align*}
  The kernel is $K(x, t) = \mathds{1}(x=t)$
  \item Constant kernel: for $C\geq 0$
  \begin{align*}
    \mu = 2\pi C\delta_0, \quad a_n = \hat{\mu}(n)=  C\int_\mathbb{T}e^{-int}
    \delta_0(t)= C
  \end{align*}
  resulting in $K(x, t) = C$
\end{itemize}

\begin{Definition}{Fourier transform on $\mathbb{R}^d$}{}
  For any $f\in L^1(\mathbb{R}^d)$ the Fourier transform of $f$ is 
  \begin{align*}
    \forall \omega \in \mathbb{R}^d, \quad \hat{f}(\omega) = 
    \int_{\mathbb{R}^d} e^{-i x^\top \omega} f(x)\text{d}x
  \end{align*}
\end{Definition}

\begin{Definition}{Fourier-Stieltjes transform}{}
  For any $\mu \in M(\mathbb{R}^d)$, the Fourier-Stieltjes transform of $\mu$ 
  is the function:

  \begin{align*}
    \forall \omega \in \mathbb{R}^d, \quad\hat{\mu}(\omega) =
    \int_{\mathbb{R}^d}e^{-i x^\top \omega}\text{d}\mu(x)
  \end{align*}
\end{Definition}


\begin{Definition}{Shift invariant kernels on $\mathbb{R}^d$}{}
  A kernel $K: \mathbb{R}^d\times \mathbb{R}^d \rightarrow\mathbb{R}$ is called 
  shift invariant (or translation invariant, t.i.) if it only depends on the 
  difference between its arguments, i.e. 
  \begin{align*}
    \forall x, y \in \mathbb{R}^d, \quad K(x, y) = \varphi(x-y)
  \end{align*}
  for some function $\varphi : \mathbb{R}^d\rightarrow \mathbb{R}$. Such a 
  function $\varphi$ is called positive definite if the corresponding kernel 
  $K$ is p.d.
\end{Definition}

\begin{Theorem}{Bochner}{bochner}
  A continuous function $\varphi : \mathbb{R}^d\rightarrow \mathbb{R}$ is p.d. 
  iff it is the Fourier-Stieltjes transform of a symmetric and positive finite
  Borel measure $\mu \in M(\mathbb{T})$
\end{Theorem}

\begin{Theorem}{RKHS of translation invariant kernels}{rkhs_ti}
  Let $K(x,t) = \varphi(x-t)$ be a translation invariant p.d. kernel  such that
  $\varphi$ is integrable on $\mathbb{R}^d$ as well as its Fourier transform 
  $\hat{\varphi}$. The subset $\mathcal{H}$ of $L^2(\mathbb{R})$ that consists
  of integrable and continuous functions $f$ such that 
  \begin{align*}
    \lVert f \rVert^2_K \triangleq \frac{1}{(2\pi)^d}\int_{\mathbb{R}^d}
    \frac{\left|\hat{f}(\omega)\right|^2}{\hat{\varphi}(\omega)}\text{d}\omega
    < +\infty
  \end{align*}
  endowed with the inner product
  \begin{align*}
    \langle f, g \rangle \triangleq \frac{1}{(2\pi)^d}\int_{\mathbb{R}^d}
    \frac{\hat{f}(\omega)\overline{\hat{g}(\omega)}}{\hat{\varphi}(\omega)}
    \text{d}\omega
  \end{align*}
  is a RKHS with $K$ as r.k.
\end{Theorem}

Examples:
\begin{itemize}
  \item Gaussian kernel:
  \begin{align*}
    K(x, y) = e^{-\frac{(x-y)^2}{2\sigma^2}}
  \end{align*}
  corresponds to $\hat{\varphi}(\omega) = e^{-\frac{\sigma^2\omega^2}{2}}$ and 
  \begin{align*}
    \mathcal{H}  = \left\{f: \int\left|\hat{f}(\omega)\right|^2 
    e^{\frac{\sigma^2\omega^2}{2}} \text{d}\omega < \infty \right\}
  \end{align*}
  In particular, all functions in $\mathcal{H}$ are infinitely differentiable 
  with all derivatives in $L^2$.

  \item Laplace kernel:
  \begin{align*}
    K(x, y) = \frac{1}{2}e^{-\gamma|x-y|}
  \end{align*}
  corresponds to $\hat{\varphi}(\omega) = \frac{\gamma}{\gamma^2 + \omega^2}$ 
  and 
  \begin{align*}
    \mathcal{H}  = \left\{f: \int\left|\hat{f}(\omega)\right|^2 
    \frac{\gamma}{\gamma^2 + \omega^2} \text{d}\omega < \infty \right\}
  \end{align*}
  the set of functions $L^2$ differentiable with derivatives in $L^2$ (Sobolev
  norm).

  \item Low frequency filter:
  \begin{align*}
    K(x, y) = \frac{\sin(\Omega(x-y))}{\pi(x-y)}
  \end{align*}
  corresponds to $\hat{\varphi}(\omega) = U(\omega + \Omega) - U(\omega - 
  \Omega)$ 
  and 
  \begin{align*}
    \mathcal{H}  = \left\{f: \int_{|\omega| > \Omega} \left|\hat{f}(\omega)
    \right|^2 \text{d}\omega = 0 \right\}
  \end{align*}
  the set of functions whose spectrum is in $[-\Omega, \Omega]$.
\end{itemize}

\subsubsection{Generalization to semigroups}
\begin{Definition}{}{}
  \begin{itemize}
    \item A semigroup $(S,\circ)$ is a nonempty set $S$ equipped with an 
    associative composition $\circ$ and a neutral element $e$.
    \item A semigroup with involution $(S,\circ, *)$ is a semigroup $(S,\circ)$
    together with a mapping $*:S\rightarrow S$ called involution satisfying:
    \begin{enumerate}
      \item $(s\circ t )^* = t^* \circ s^*$ for $s,t\in S$.
      \item $(s^*)^* = s$ for $s\in S$.
    \end{enumerate}
  \end{itemize}
  
  Exemples:
  \begin{itemize}
    \item A group $(G, \circ)$ is a semigroup with involution with $s^* = 
    s^{-1}$.
    \item Any abelian semigroup $(S, +)$ is a semigroup with involution with 
    identity as involution.
  \end{itemize}
\end{Definition}

\subsection{Kernels for probabilistic models}
\subsubsection{Fisher kernel}

\begin{Definition}{Fisher kernel}{}
  Fix a parameter $\theta_0 \in \Theta$ (obtained for instance by maximum 
  likelihood over a training set).

  For each sequence $x$, compute the Fisher score vector
  \begin{align*}
    \Phi_{\theta_0}(x) = \nabla_\theta \log P_\theta(x)|_{\theta=\theta_0}
  \end{align*}
  which can be interpreted as the local contribution of each parameter.

  Form the kernel
  \begin{align*}
    K(x, x') = \Phi_{\theta_0}(x)^\top I(\theta_0)^{-1}\Phi_{\theta_0}(x')
  \end{align*}
  where $I(\theta_0) = \mathbb{E}[\Phi_{\theta_0}(x)\Phi_{\theta_0}(x)^\top]$ is 
  the Fisher information matrix.
\end{Definition}
\begin{itemize}
  \item Describes how each parameter contributes to generating an example
  \item Invariant under change of parametrization 
\end{itemize}
In practice,
\begin{itemize}
  \item $\Phi_{\theta_0}(x)$ can be computed explicitly for many models (HMMs)
  estimated from data.
  \item $I(\theta_0)$ is often replaced by the identity matrix. 
  \item Several different models (i.e., different $\theta_0$) can be trained and
  combined.
  \item Fisher vectors $\varphi_{\theta_0}(x) = 
  I(\theta_0)^{-1/2}\Phi_{\theta_0}(x)$ and correspond to the explicit 
  embedding 
  \begin{align}
    K(x, x') = \varphi_{\theta_0}(x)^\top \varphi_{\theta_0}(x')
  \end{align}
\end{itemize}

Example: Gaussian model 

\subsection{Kernels for biological sequences}
\subsection{Kernels for graphs}
\subsection{Kernels on graphs}

%%---------------------------------
\section{Open Problems and Research Topics}


%%---------------------------------
%%---------------------------------

\appendix

\section{Proofs}

\subsection{Kernels and RKHS}

\begin{Proof}{of Theorem \ref{thm:rkhs}}{rkhs}
  ($\Rightarrow$) If a r.k. exists in $\mathcal{H}$ then for any $(x,f) \in 
  \mathcal{X} \times \mathcal{H}$:
  \begin{align*}
    |f(x)| &= |\langle f, K_x \rangle_\mathcal{H} |\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot \lVert K_x \rVert_\mathcal{H}
           \tag{Cauchy-Schwarz}\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot K(x,x)^\frac{1}{2}
  \end{align*}
  Therefore, $f\in\mathcal{H} \rightarrow f(x) \in \mathbb{R}$ is a linear 
  continuous mapping because $F$ is linear and $\lim_{f\rightarrow 0} F(f)= 0$

  \vspace{10pt}
  ($\Leftarrow$) $F$ is continuous, by the Riesz representation theorem: there
  exists a unique $g_x \in \mathcal{H}$ such that $f(x) = \langle f, g_x 
  \rangle_\mathcal{H}$.

  The function $K: (x, y) \mapsto g_x(y)$ is then a r.k. for $\mathcal{H}$
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:unique_rkhs}}{unique_rkhs}
  (Uniqueness) If $K$ and $K'$ are two r.k. of a RKHS, then for any $x$
  \begin{align*}
    \lVert K_x - K'_x\rVert^2 = K_x(x) - K'_x(x) - K_x(x) + K'_x(x) = 0
  \end{align*}
  So $K_x = K'_X$

  % TODO The RKHS of a r.k is unique
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:pdrk}}{pdrk}
  ($\Leftarrow$) A r.k. is symmetric, and $\sum_{i,j} a_i a_j K(x_i, x_j)
   = \left\lVert \sum_i a_i K_{x_i} \right\rVert^2_\mathcal{H} \geq 0$

  \vspace{10pt}
  ($\Rightarrow$) Let $\mathcal{H}_0$ be the subspace spanned by the functions 
  $(K_x)_{x\in \mathcal{X}}$. If $f = \sum_i a_i K_{x_i}$ and $g =
  \sum_j b_j K_{y_j}$. Let (not an inner product yet)
  \begin{align*}
    \langle f, g\rangle_{\mathcal{H}_0} &= \sum_{i,j}a_i b_j K(x_i, y_j)\\
    &= \sum_i a_i g(x_i)\\
    &= \sum_j b_j f(y_j)
  \end{align*}
  ($\langle f,g \rangle_{\mathcal{H}_0}$ does not depend on the expansion of $f$
  or $g$) For any $x\in \mathcal{X}$ and $f \in \mathcal{H}_0$, $\langle f,K_x 
  \rangle_{\mathcal{H}_0} = f(x)$.

  \begin{align*}
    \lVert f \rVert^2_{\mathcal{H}_0} = \sum_{i,j} a_i a_j K(x_i, x_j) \geq 0
  \end{align*}
  And since Cauchy-Schwarz is valid, 
  \begin{align*}
    |f(x)| = |\langle f, K_x \rangle_{\mathcal{H}_0}| \leq \lVert f 
    \rVert_{\mathcal{H}_0}\cdot K(x,x)^{\frac{1}{2}}
  \end{align*}
  Therefore $\lVert f \rVert_{\mathcal{H}_0} = 0 \implies f = 0$. $\langle ., .
  \rangle$ is an inner product on $\mathcal{H}_0$. 

  For a Cauchy sequence $(f_n)_{n\geq 0}$, 
  \begin{align*}
    |f_m(x) - f_n(x)| \leq \lVert f_m - f_n \rVert_{\mathcal{H}_0} \cdot
    K(x, x)^{\frac{1}{2}}
  \end{align*}
  For any $x$ the sequence $(f_n(x))$ is Cauchy in $\mathbb{R}$ and therefore
  converges. 

  If the functions defined as the point-wise limits of Cauchy sequences are
  added $\mathcal{H}_0$, it becomes a Hilbert space with $K$ as r.k..
\end{Proof}

\begin{Proof}{of \hyperref[thm:aronszajn]{Aronszajn's theorem}}{aronszajn}
  If $K$ is p.d. over a set $\mathcal{X}$, it is the r.k. of a Hilbert space 
  $\mathcal{H}$. The mapping $\Phi$ is defined by $\forall x \in \mathcal{X}, 
  \quad \Phi(x) = K_x$.

  By the reproducing property 
  \begin{align*}
    \forall (x,y)\in \mathcal{X}^2,\quad \langle \Phi(x), \Phi(y) 
    \rangle_\mathcal{X} = \langle K_x, K_y \rangle_\mathcal{X} = K(x, y)
  \end{align*}
\end{Proof}

\subsection{Kernels Tricks}

\begin{Proof}{of the \hyperref[thm:representer]{Representer theorem}}
  {representer}
  Let $\xi(f)$ the functional that is minimized in the optimization problem of 
  the  theorem, and $\mathcal{H}_\mathcal{S}$ the linear span of all the 
  $K_{x_i}$ functions. 
  
  Since $\mathcal{H}_\mathcal{S}$ is a finite dimensional space, every function 
  $f\in\mathcal{H}$ can be decomposed as $f = f_\mathcal{S} + f_\perp$, with 
  $f_S$ the orthogonal projection of f on $\mathcal{H}_\mathcal{S}$.

  Because $\mathcal{H}$ is a RKHS, 
  \begin{align*}
    \forall i \leq n,\quad f_\perp(x_i) = \langle f_\perp, K_{x_i} 
    \rangle_\mathcal{H} = 0
  \end{align*}
  Therefore 
  \begin{align*}
    \forall  i \leq n,\quad f(x_i) = f_\mathcal{S}(x_i)
  \end{align*}
  From Pythagora's theorem in $\mathcal{H}$, $\lVert f\rVert^2_\mathcal{H} = 
  \lVert f_\mathcal{S}\rVert^2_\mathcal{H} + \lVert f_\perp
  \rVert^2_\mathcal{H}$.

  We therefore have $\xi(f) \geq \xi(f_\mathcal{S})$ with equality if and only
  if $\lVert f_\perp\rVert^2_\mathcal{H} = 0$, the minimum belongs to 
  $\mathcal{H}_\mathcal{S}$.
\end{Proof}

\subsection{Kernel Methods: Unsupervised Learning}

\begin{Proof}{of Proposition \ref{pro:center}}{center}
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \lVert \varphi(x_i) - \mu \rVert^2_\mathcal{H} & =
    \frac{1}{n}\sum_{i=1}^n \lVert \varphi(x_i)\rVert^2_\mathcal{H} - \left
    \langle \frac{2}{n}\sum_{i=1}^n\varphi(x_i), \mu \right\rangle_\mathcal{H} 
    + \lVert\mu \rVert^2_\mathcal{H}\\
    & = \frac{1}{n}\sum_{i=1}^n \lVert \varphi(x_i)\rVert^2_\mathcal{H} - 2 
    \langle \varphi_n, \mu \rangle_\mathcal{H} + \lVert\mu \rVert^2_\mathcal{H}
    \\
    & = \frac{1}{n}\sum_{i=1}^n \lVert \varphi(x_i)\rVert^2_\mathcal{H} - \lVert
    \varphi_n \rVert^2_\mathcal{H} + \lVert \varphi_n - \mu \rVert^2_\mathcal{H}
  \end{align*}
  which is minimum for $\varphi_n = \mu$.
\end{Proof}

\subsection{The Kernel Jungle}

\begin{Proof}{of Theorem \ref{thm:green1}}{green1}
  
\end{Proof}

\end{document}
