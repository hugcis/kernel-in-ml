\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{boxedthms}
\usepackage{hyperref}

\title{Kernel Methods in Machine Learning - Course Notes}
\author{Hugo Cisneros}
\date{}

\begin{document}
\maketitle
\setcounter{tocdepth}{2}
\tableofcontents


\section{Kernels and RKHS}

\subsection{Positive Definite Kernels}

\begin{Definition}{}{}
    A kernel $K$ is  a comparison function $K: \mathcal{X}\times\mathcal{X}
 \rightarrow \mathbb{R}$.


    With $n$ data point $\{x_1, x_2, ..., x_n\}$ a $n \times n$ matrix 
    $\mathbf{K}$ can be defined by $\mathbf{K}_{ij} = K(x_i, x_j)$.

    A kernel $K$ is \textbf{positive definite} (p.d.) if it is 
    \textbf{symmetric} ($K(x, x') = K(x', x)$) and for all sets of $a$ and $x$
    \begin{align*}
        \boxed{\sum_i\sum_j a_i a_j K(x_i, x_j) \geq 0}
    \end{align*}
\end{Definition}

This is equivalent to the kernel matrix being \textbf{positive semi-definite}. 

\underline{Examples:}\begin{itemize}
    \item Kernel on $\mathbb{R}\times\mathbb{R}$  defined by 
    $K(x, x') = xx'$ is p.d. ($xx' = x'x$ and $ \sum_i\sum_j a_i a_j 
    K(x_i, x_j) = \left(\sum_i a_i x_i\right)^2 \geq 0$).
    \item Linear kernel ($K(x, x') = \langle x, x'\rangle_{\mathbb{R}^d}$) is 
    p.d 
    \item More generally for any set $\mathcal{X}$, and function $\Phi: 
    \mathcal{X} \rightarrow \mathbb{R}^d$, the kernel defined by $K(x,x') = 
    \langle \Phi(x), \Phi(x') \rangle_{\mathbb{R}^d}$ is p.d.
\end{itemize} 

\begin{Theorem}{Aronszajn, 1950}{aronszajn}
    $K$ is a p.d. kernel on the set $\mathcal{X}$ if and only if there exists a 
    \textbf{Hilbert space $\mathcal{H}$ and a mapping $\Phi : \mathcal{X} 
    \rightarrow \mathcal{H}$} such that, for any $x$, $x'$ in $\mathcal{X}$:
    \begin{align*}
        \boxed{K(x, x') = \langle \Phi(x), \Phi(x') \rangle_\mathcal{H}}
    \end{align*}
    \par \hfill \hyperref[prf:aronszajn]{\small Proof}.
\end{Theorem}


(A Hilbert space is a vector space with an inner product and complete for the 
corresponding norm).

\subsection{Reproducing Kernel Hilbert Spaces (RKHS)}

Let $\mathcal{X}$ be a set and $\mathcal{H} \subset \mathbb{R}^\mathcal{X}$ a 
class of functions forming a Hilbert space. 
\begin{Definition}{Reproducing kernel}{}
    A kernel $K$ is called a \textbf{reproducing kernel} (r.k.) of $\mathcal{H}$ 
    if 
    \begin{itemize}
        \item $\mathcal{H}$ contains all functions of the form 
        \begin{align*}
           \boxed{\forall x \in \mathcal{X}, K_x: t \rightarrow K(x, t)}
        \end{align*}
        \item For every $x \in \mathcal{X}$ and $f\in \mathcal{H}$, 
        $\boxed{f(x) = \langle f, K_x \rangle_\mathcal{H} }$
    \end{itemize}
\end{Definition}

If there exists a r.k., $\mathcal{H}$ is called a RKHS.

\begin{Theorem}{Equivalent Definition of RKHS}{rkhs}
    $\mathcal{H}$ is a RKHS if and only if for any $x\in\mathcal{X}$, the 
    mapping
    \begin{align*}
        F: & \mathcal{H}\rightarrow \mathbb{R}\\
        & f \mapsto f(x)
    \end{align*}
    is \textbf{continuous}.
    \par \hfill \hyperref[prf:rkhs]{\small Proof}.
\end{Theorem}


As a corollary, convergence in a RKHS implies point-wise convergence.


\begin{Theorem}{Uniqueness of RKHS}{unique_rkhs}
  If $\mathcal{H}$ is a RKHS, it has a \textbf{unique r.k.}, and a function $K$
  can be \textbf{the r.k of at most one RKHS}.
  \par \hfill \hyperref[prf:unique_rkhs]{\small Proof}.
\end{Theorem}



\begin{Theorem}{}{pdrk}
  A function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is 
  \textbf{p.d. if and only if it is a r.k.}.
  \par \hfill \hyperref[prf:pdrk]{\small Proof}.
\end{Theorem}



\subsection{Examples}

\subsubsection{Steps for finding the RKHS of a Kernel}
\begin{enumerate}
  \item Look for an \textbf{inner product} ($K(x, y) = \langle \Phi(x), \Phi(y)
  \rangle_\mathcal{H}$)
  \item Propose a \textbf{candidate RKHS} $\mathcal{H}$
  \item Check that the candidate $\mathcal{H}$ is a \textbf{Hilbert space} (
  inner product and complete)
  \item Check that $\mathcal{H}$ is \textbf{the RKHS}
  \begin{itemize}
    \item $\mathcal{H}$ contains all the functions $K_x : t \mapsto K(x,t)$
    \item For all $f \in \mathcal{H}$ and $x\in \mathcal{X}$, $f(x) = \langle
    f, K_x \rangle_\mathcal{H}$.
  \end{itemize}
\end{enumerate}

\subsubsection{Linear Kernel}

\begin{Definition}{Linear Kernel}{}
  In $\mathbb{R}^d$, the linear kernel is defined by $K(x, y) = \langle x, y 
  \rangle_{\mathbb{R}^d}$
\end{Definition}

\begin{Theorem}{RKHS of a linear Kernel}{lin_rkhs}
  The RKHS of the linear kernel is the set of linear functions of the form 
  $f_w(x) = \langle w,x \rangle_{\mathbb{R}^d}$ for $w \in \mathbb{R}^d$,
  endowed with the inner product $\langle f_w,f_v \rangle_\mathcal{H} = 
  \langle w,v \rangle_{\mathbb{R}^d}$
\end{Theorem}

\subsubsection{Polynomial Kernel}

\begin{Definition}{Polynomial Kernel}{}
  In $\mathbb{R}^d$, the polynomial kernel is defined by $K(x, y) = \langle x, y 
  \rangle_{\mathbb{R}^d}^2$
\end{Definition}

\begin{Theorem}{RKHS of a polynomial Kernel}{poly_rkhs}
  The RKHS $\mathcal{H}$ of the polynomial kernel is the set of quadratic 
  functions of the form $f_S(x) = x^T S x$ for $S \in \mathcal{S}^{d\times d}$ 
\end{Theorem}

\subsubsection{Properties of kernels} 

If $K_1$, $K_2$ are p.d. kernels, 
\begin{itemize}
  \item $K_1 + K_2$ is a p.d. kernel
  \item $K_1\cdot K_2$ is a p.d. kernel
  \item $cK_1$ for $c\geq 0$ is a p.d. kernel
  \item The point-wise limits of a sequence of p.d. kernels is a p.d kernel.
  \item $\exp(K_1)$ is a p.d. kernel
\end{itemize}

\textbf{Small norms in the RKHS space means slow variations in the original
space $\mathcal{X}$ with respect to the geometry defined by the kernel.}
%%---------------------------------
\section{Kernel tricks}

\subsection{Kernel trick}
\textbf{Statement:} All expression of vectors that can be written in terms of 
pairwise inner products can be transposed to a infinite dimensional space by 
replacing inner products with kernel evaluations. 

\subsection{Representer theorem}

\begin{Theorem}{Representer theorem}{representer}
  Let $\mathcal{X}$ a set with a p.d. kernel $K$ and corresponding RKHS 
  $\mathcal{H}$, $S = \{x_1, ..., x_n\} \subset \mathcal{X}$ a set of points of 
  $\mathcal{X}$.

  Let $\Phi : \mathbb{R}^{n+1} \rightarrow \mathbb{R}$ a function strictly 
  increasing w.r.t. the last variable.

  Any solution to the optimization problem
  \begin{align*}
    \min_{f\in \mathcal{H}} \Phi(f(x_1), ..., f(x_n), \lVert f
    \rVert_\mathcal{H})
  \end{align*}
  admits a representation in the form 
  \begin{align*}
    \forall x \in \mathcal{X}, f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)
  \end{align*}

  \par\hfill \hyperref[prf:representer]{\small Proof}
\end{Theorem}

One of the main consequences of the theorem is that problems of the form 
\begin{align*}
  \min_{f\in \mathcal{H}} \Phi(f(x_1), ..., f(x_n), \lVert f
  \rVert_\mathcal{H})
\end{align*}
can be re-written as 
\begin{align*}
  \min_{\alpha\in \mathbb{R}^n} \Phi([\mathbf{K}\alpha]_1, ..., 
  [\mathbf{K}\alpha]_n, \alpha^T \mathbf{K} \alpha)
\end{align*}
which is a n-dimensional optimization problem (instead of a possibly infinite 
dimensional one).

%%---------------------------------
\section{Kernel Methods: Supervised Learning}

\subsection{Kernel Ridge regression}

The problem can be described as minimizing a RKHS norm regularized MSE criterion
\begin{align*}
  \hat{f} = \arg\min_{f\in \mathcal{H}} \frac{1}{n}\sum_{i=1}^n(y_i - f(x_i))^2
  + \lambda\lVert f\rVert^2_\mathcal{H}
\end{align*} 

Effects of regularization: 
\begin{itemize}
  \item \textbf{Penalize non smooth functions} (avoid overfitting)
  \item \textbf{Simplify the solution} (representer theorem)
\end{itemize}

The problem can be re-written 
\begin{align*}
  \hat{f} = \arg\min_{\alpha \in \mathbb{R}^n}\frac{1}{n}
  (\mathbf{K}\alpha - y)^T (\mathbf{K}\alpha - y) + \lambda\alpha^T\mathbf{K}
  \alpha
\end{align*}

One solution is to take 
\begin{align*}
  \alpha = (\mathbf{K} + \lambda n \mathbf{I})^{-1} y
\end{align*}

(Uniqueness: If $\mathbf{K}$ is singular, all $\alpha + \varepsilon$ with 
$\varepsilon \in \text{Ker}(\mathbf{K})$ are solutions leading to the same 
function $f$.)

\subsection{Kernel logistic regression}



\subsection{Largse-margin classifiers}

%%---------------------------------
\section{Kernel Methods: Unsupervised Learning}

%%---------------------------------
\section{The Kernel Jungle}
\subsection{Green, Mercer, Herglotz, Bochner and friends}

\subsubsection{Green Kernel}

\begin{Theorem}{Green Kernel in dimension 1}{green1}
  The set defined by
  \begin{align*}
    \mathcal{H} = \left\{f: [0, 1] \rightarrow \mathbb{R}, \text{absolutely 
    continuous}, f' \in L^2([0, 1]), f(0) = 0 \right\}
  \end{align*}
  endowed with the inner product $\forall (f, g) \in \mathcal{F}^2 \langle f, g
  \rangle = \int_0^1 f'(u)g'(u)du$, 
  is a RKHS with with r.k.
  \begin{align*}
    \forall (x, y) \in [0, 1]^2, K(x, y) = \min(x, y)
  \end{align*}.
\end{Theorem}

\begin{Theorem}{General Green Kernel}{green_general}
  If $D$ is a differential operator on a class of functions of $\mathcal{H}$
  such that the inner product $\langle f, g \rangle_\mathcal{H} = \langle Df, Dg 
  \rangle_{L^2(\mathcal{X})}$

  Then $\mathcal{H}$ is a RKHS and admits for r.k. the Green function of the 
  operator $D^*D$
\end{Theorem}


\subsubsection{Mercer Kernels}

\begin{Definition}{Mercer Kernels}{}
  A kernel $K$ on a set $\mathcal{X}$ is called a Mercer kernel if: 
  \begin{itemize}
    \item $\mathcal{X}$ is a compact metric space (typically, a closed bounded
    subset of $\mathbb{R}^d$)
    \item $K: \mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}$ is a
    continuous p.d kernel (w.r.t the Borel topology)
  \end{itemize}
\end{Definition}

%%---------------------------------
\section{Open Problems and Research Topics}


%%---------------------------------
%%---------------------------------

\appendix

\section{Proofs}

\subsection{Kernels and RKHS}

\begin{Proof}{of Theorem \ref{thm:rkhs}}{rkhs}
  ($\Rightarrow$) If a r.k. exists in $\mathcal{H}$ then for any $(x,f) \in 
  \mathcal{X} \times \mathcal{H}$:
  \begin{align*}
    |f(x)| &= |\langle f, K_x \rangle_\mathcal{H} |\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot \lVert K_x \rVert_\mathcal{H}
           \tag{Cauchy-Schwarz}\\
           &\leq \lVert f \rVert_\mathcal{H} \cdot K(x,x)^\frac{1}{2}
  \end{align*}
  Therefore, $f\in\mathcal{H} \rightarrow f(x) \in \mathbb{R}$ is a linear 
  continuous mapping because $F$ is linear and $\lim_{f\rightarrow 0} F(f)= 0$

  \vspace{10pt}
  ($\Leftarrow$) $F$ is continuous, by the Riesz representation theorem: there
  exists a unique $g_x \in \mathcal{H}$ such that $f(x) = \langle f, g_x 
  \rangle_\mathcal{H}$.

  The function $K: (x, y) \mapsto g_x(y)$ is then a r.k. for $\mathcal{H}$
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:unique_rkhs}}{unique_rkhs}
  (Uniqueness) If $K$ and $K'$ are two r.k. of a RKHS, then for any $x$
  \begin{align*}
    \lVert K_x - K'_x\rVert^2 = K_x(x) - K'_x(x) - K_x(x) + K'_x(x) = 0
  \end{align*}
  So $K_x = K'_X$

  % TODO The RKHS of a r.k is unique
\end{Proof}

\begin{Proof}{of Theorem \ref{thm:pdrk}}{pdrk}
  ($\Leftarrow$) A r.k. is symmetric, and $\sum_{i,j} a_i a_j K(x_i, x_j)
   = \left\lVert \sum_i a_i K_{x_i} \right\rVert^2_\mathcal{H} \geq 0$

  \vspace{10pt}
  ($\Rightarrow$) Let $\mathcal{H}_0$ be the subspace spanned by the functions 
  $(K_x)_{x\in \mathcal{X}}$. If $f = \sum_i a_i K_{x_i}$ and $g =
  \sum_j b_j K_{y_j}$. Let (not an inner product yet)
  \begin{align*}
    \langle f, g\rangle_{\mathcal{H}_0} &= \sum_{i,j}a_i b_j K(x_i, y_j)\\
    &= \sum_i a_i g(x_i)\\
    &= \sum_j b_j f(y_j)
  \end{align*}
  ($\langle f,g \rangle_{\mathcal{H}_0}$ does not depend on the expansion of $f$
  or $g$) For any $x\in \mathcal{X}$ and $f \in \mathcal{H}_0$, $\langle f,K_x 
  \rangle_{\mathcal{H}_0} = f(x)$.

  \begin{align*}
    \lVert f \rVert^2_{\mathcal{H}_0} = \sum_{i,j} a_i a_j K(x_i, x_j) \geq 0
  \end{align*}
  And since Cauchy-Schwarz is valid, 
  \begin{align*}
    |f(x)| = |\langle f, K_x \rangle_{\mathcal{H}_0}| \leq \lVert f 
    \rVert_{\mathcal{H}_0}\cdot K(x,x)^{\frac{1}{2}}
  \end{align*}
  Therefore $\lVert f \rVert_{\mathcal{H}_0} = 0 \implies f = 0$. $\langle ., .
  \rangle$ is an inner product on $\mathcal{H}_0$. 

  For a Cauchy sequence $(f_n)_{n\geq 0}$, 
  \begin{align*}
    |f_m(x) - f_n(x)| \leq \lVert f_m - f_n \rVert_{\mathcal{H}_0} \cdot
    K(x, x)^{\frac{1}{2}}
  \end{align*}
  For any $x$ the sequence $(f_n(x))$ is Cauchy in $\mathbb{R}$ and therefore
  converges. 

  If the functions defined as the point-wise limits of Cauchy sequences are
  added $\mathcal{H}_0$, it becomes a Hilbert space with $K$ as r.k..
\end{Proof}

\begin{Proof}{of \hyperref[thm:aronszajn]{Aronszajn's theorem}}{aronszajn}
  If $K$ is p.d. over a set $\mathcal{X}$, it is the r.k. of a Hilbert space 
  $\mathcal{H}$. The mapping $\Phi$ is defined by $\forall x \in \mathcal{X}, 
  \quad \Phi(x) = K_x$.

  By the reproducing property 
  \begin{align*}
    \forall (x,y)\in \mathcal{X}^2,\quad \langle \Phi(x), \Phi(y) 
    \rangle_\mathcal{X} = \langle K_x, K_y \rangle_\mathcal{X} = K(x, y)
  \end{align*}
\end{Proof}

\begin{Proof}{of the \hyperref[thm:representer]{Representer theorem}}
  {representer}
  Let $\xi(f)$ the functional that is minimized in the optimization problem of 
  the  theorem, and $\mathcal{H}_\mathcal{S}$ the linear span of all the 
  $K_{x_i}$ functions. 
  
  Since $\mathcal{H}_\mathcal{S}$ is a finite dimensional space, every function 
  $f\in\mathcal{H}$ can be decomposed as $f = f_\mathcal{S} + f_\perp$, with 
  $f_S$ the orthogonal projection of f on $\mathcal{H}_\mathcal{S}$.

  Because $\mathcal{H}$ is a RKHS, 
  \begin{align*}
    \forall i \leq n,\quad f_\perp(x_i) = \langle f_\perp, K_{x_i} 
    \rangle_\mathcal{H} = 0
  \end{align*}
  Therefore 
  \begin{align*}
    \forall  i \leq n,\quad f(x_i) = f_\mathcal{S}(x_i)
  \end{align*}
  From Pythagora's theorem in $\mathcal{H}$, $\lVert f\rVert^2_\mathcal{H} = 
  \lVert f_\mathcal{S}\rVert^2_\mathcal{H} + \lVert f_\perp
  \rVert^2_\mathcal{H}$.

  We therefore have $\xi(f) \geq \xi(f_\mathcal{S})$ with equality if and only
  if $\lVert f_\perp\rVert^2_\mathcal{H} = 0$, the minimum belongs to 
  $\mathcal{H}_\mathcal{S}$.
\end{Proof}

\subsection{The Kernel Jungle}

\begin{Proof}{}{}
  
\end{Proof}

\end{document}
